To achieve this, we will need several Python files and libraries. The core libraries we will use are pandas for data manipulation, sklearn for machine learning, and streamlit for the user interface. We will also use numpy for numerical operations and seaborn for data visualization.

Here are the files we will need:

1. main.py: This is the entry point of our application. It will handle the user interface and call the appropriate functions based on user input.

2. data_cleaning.py: This file will contain functions for cleaning the dataset.

3. feature_engineering.py: This file will contain functions for feature engineering.

4. model.py: This file will contain functions for training the regression model and calculating RMSE and R2.

5. requirements.txt: This file will list all the Python libraries that our application depends on.

Let's start with the main.py file:

main.py
```python
import streamlit as st
import pandas as pd
from data_cleaning import clean_data
from feature_engineering import engineer_features
from model import train_model, calculate_metrics

def main():
    st.title('Regression Model Trainer')
    
    uploaded_file = st.file_uploader("Upload your input CSV file", type=["csv"])
    if uploaded_file is not None:
        input_df = pd.read_csv(uploaded_file)
        st.write(input_df)
        
        st.subheader('Data Cleaning')
        cleaned_df = clean_data(input_df)
        st.write(cleaned_df)
        
        st.subheader('Feature Engineering')
        features_df = engineer_features(cleaned_df)
        st.write(features_df)
        
        st.subheader('Model Training')
        model = train_model(features_df)
        
        st.subheader('Model Metrics')
        rmse, r2 = calculate_metrics(model, features_df)
        st.write(f'RMSE: {rmse}')
        st.write(f'R2: {r2}')

if __name__ == "__main__":
    main()
```

Next, let's create the data_cleaning.py file:

data_cleaning.py
```python
import pandas as pd

def clean_data(df):
    # Implement your data cleaning logic here
    cleaned_df = df.dropna()  # For simplicity, we just drop rows with missing values
    return cleaned_df
```

Then, the feature_engineering.py file:

feature_engineering.py
```python
import pandas as pd

def engineer_features(df):
    # Implement your feature engineering logic here
    features_df = df  # For simplicity, we don't do any feature engineering
    return features_df
```

Next, the model.py file:

model.py
```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

def train_model(df):
    X = df.drop('target', axis=1)
    y = df['target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model = LinearRegression()
    model.fit(X_train, y_train)
    return model

def calculate_metrics(model, df):
    X = df.drop('target', axis=1)
    y = df['target']
    y_pred = model.predict(X)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    r2 = r2_score(y, y_pred)
    return rmse, r2
```

Finally, the requirements.txt file:

requirements.txt
```
streamlit
pandas
numpy
scikit-learn
```

To run the application, you can use the following command in the terminal:

```
streamlit run main.py
```

This concludes a fully working implementation.